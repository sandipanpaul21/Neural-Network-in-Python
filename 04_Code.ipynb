{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNOyU0KS60lPNHxnz8MqvzW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanpaul21/Neural-Network-in-Python/blob/main/04_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Activation Function\n",
        "\n",
        "- The activation function is applied to the output of a neuron (or layer of neurons), which modifies outputs. \n",
        "- We use activation functions because if the activation function itself is nonlinear, it allows for neural networks with usually two or more hidden layers to map nonlinear functions. \n",
        "- In general, your neural network will have two types of activation functions. \n",
        "  1. The first will be the activation function used in hidden layers\n",
        "  2. The second will be used in the output layer. \n",
        "  \n",
        "  Usually, the activation function used for hidden neurons will be the same for all of them, but it doesn’t have to.\n",
        "\n",
        "#### Types of Activation Function\n",
        "1. Step Activation Function\n",
        "2. Sigmoid Activation Function\n",
        "3. Linear Activation Function\n",
        "4. Rectifier Linear Activation Function\n",
        "5. Softmax Activation Function\n",
        "\n",
        "#### 1. The Step Activation Function\n",
        "- Recall the purpose this activation function serves is to mimic a neuron “firing” or “not firing” based on input information. \n",
        "- The simplest version of this is a step function. \n",
        "- Formula wise, \n",
        "        y = 1 if output > 0 else 0 where output = sum(weights * inputs) + bias\n",
        "- In a single neuron, \n",
        "  - Output​ > 0 then Output of Neuron is 1\n",
        "  - otherwise, it will output 0.\n",
        "\n",
        "#### 2. The Sigmoid Activation Function\n",
        "- The problem with the step function is it’s not very informative. \n",
        "- The problem with a step function is \n",
        "  - It’s give less clear to the optimizer what these impacts are because there’s very little information gathered from this function. \n",
        "  - It’s either on (1) or off (0). \n",
        "  - It’s hard to tell how “close” this step function was to activating or deactivating. Maybe it was very close, or maybe it was very far. \n",
        "- In terms of the final output value from the network, it doesn’t matter if it was ​close​ to outputing something else. \n",
        "- Thus, when it comes time to optimize weights and biases, it’s easier for the optimizer if we have activation functions that are more granular and informative.\n",
        "- The original, more granular, activation function used for neural networks was the ​Sigmoid activation function, which looks like:\n",
        "        y = 1 / (1 + e^-x) # e raise to -x\n",
        "  - This function \n",
        "    - Range is from 0 to 1\n",
        "    - Returns a value in the range of 0 for negative infinity, \n",
        "    - Through 0.5 for the input of 0, \n",
        "    - To 1 for positive infinity.\n",
        "\n",
        "#### 3. The Linear Activation Function\n",
        "- A linear function is simply the equation of a line. \n",
        "- It will appear as a straight line when graphed, \n",
        "        y=x here, the output value equals the input.\n",
        "\n",
        "#### 4. The Rectified Linear Activation Function (ReLU)\n",
        "- ReLU means Rectified Linear Units​ activation function\n",
        "- The rectified linear activation function is simpler than the sigmoid. It’s quite literally ​y=x,​ clipped at 0 from the negative side. If ​x​ is less than or equal to ​0,​ then ​y​ is ​0​ — otherwise, ​y​ is equal to ​x​.\n",
        "        y = x if x > 0 else 0 if x <= 0\n",
        "  - This function\n",
        "    - Will return x if x > 0\n",
        "    - Else return 0 if x < = 0\n",
        "\n",
        "#### 5. The Softmax Activation Function\n",
        "- If model is to be a classifier, so we want an activation function meant for classification. One of these is the Softmax activation function.  \n",
        "- In the case of classification, what we want to see is a prediction of which class the network \"classify the input represents. \n",
        "- This distribution returned by the softmax activation function represents ​confidence scores​ for each class and will add up to 1.\n",
        "-  The predicted class is associated with the output neuron that returned the largest confidence score. \n",
        "\n",
        "        y = e^z / sum(e^z)\n",
        "  - This function\n",
        "    - will return probability of each class and will add to 1\n",
        "\n",
        "#### Why Use Activation Functions?\n",
        "- In most cases, for a neural network to fit a nonlinear function, we need it to contain two or more hidden layers, and we need those hidden layers to use a nonlinear activation function.\n",
        "- A nonlinear function cannot be represented well by a straight line, such as a sine function.\n",
        "- Most of the problems in world are non linear. So to fit a model to a non linear data we use Activation Function like ReLU which help the model to fit to an Non Linear Data."
      ],
      "metadata": {
        "id": "Vy7iv2ZcMxHp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PPQxn9GL9Mi",
        "outputId": "cbd3d95d-5b52-4247-ed70-768fda45e69d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "input : -1\n",
            "Value greater than 0 then input else 0 : 0\n",
            "Output after ReLU: [0]\n",
            "\n",
            "input : 0\n",
            "Value greater than 0 then input else 0 : 0\n",
            "Output after ReLU: [0, 0]\n",
            "\n",
            "input : 1\n",
            "Value greater than 0 then input else 0 : 1\n",
            "Output after ReLU: [0, 0, 1]\n",
            "\n",
            "input : 2\n",
            "Value greater than 0 then input else 0 : 2\n",
            "Output after ReLU: [0, 0, 1, 2]\n",
            "\n",
            "input : 3\n",
            "Value greater than 0 then input else 0 : 3\n",
            "Output after ReLU: [0, 0, 1, 2, 3]\n",
            "\n",
            "input : 4\n",
            "Value greater than 0 then input else 0 : 4\n",
            "Output after ReLU: [0, 0, 1, 2, 3, 4]\n",
            "\n",
            "input : -4\n",
            "Value greater than 0 then input else 0 : 0\n",
            "Output after ReLU: [0, 0, 1, 2, 3, 4, 0]\n",
            "\n",
            "input : -5\n",
            "Value greater than 0 then input else 0 : 0\n",
            "Output after ReLU: [0, 0, 1, 2, 3, 4, 0, 0]\n",
            "\n",
            "Final Output after ReLU : [0, 0, 1, 2, 3, 4, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "# ReLU Activation Code\n",
        "# he ReLU in this code is a loop where we’re checking if the current value is greater than 0. \n",
        "# If it is, we’re appending it to the output list, and if it’s not, we’re appending 0. \n",
        "# This can be written more simply, as we just need to take the largest of two values: 0 or neuron value. \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "input = [-1,0,1,2,3,4,-4,-5]\n",
        "output = []\n",
        "\n",
        "for i in input:\n",
        "  print(\"\\ninput :\",i)\n",
        "  print(\"Value greater than 0 then input else 0 :\",max(0,i))\n",
        "  output.append(max(0,i))\n",
        "  print(\"Output after ReLU:\",output)\n",
        "\n",
        "print(\"\\nFinal Output after ReLU :\",output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternate Way using NumPy\n",
        "\n",
        "input = [-1,0,1,2,3,4,-4,-5]\n",
        "output = np.maximum(0,input)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3o5vj6HSiZL",
        "outputId": "caf2e30a-778d-4ee7-8a04-06ed963e2320"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 2 3 4 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets create and generalize ReLU activation function\n",
        "\n",
        "# Relu Activation\n",
        "class Activation_ReLU:\n",
        "\n",
        "  # Forward Pass\n",
        "  def Forward(self, inputs):\n",
        "    self.output = np.maximum(0,input)"
      ],
      "metadata": {
        "id": "i5p7XTfDTngo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets create a sample Training Dataset (Random Set)\n",
        "import nnfs   # Will help to create random spiral dataset\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()   # Will set random seed = 0, to make it repeatable\n",
        "X, Y = spiral_data(samples = 2,classes = 3)\n",
        "print(\"Data is in X Variable \")\n",
        "print(X)    # 3 Clases created with 2 sample each\n",
        "print(\"\\nTarget Variable or Classes Defined are in Y Variable\") \n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-4IY4sxUGz7",
        "outputId": "411b99d6-795b-4575-ea7e-73f57ea1b7fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data is in X Variable \n",
            "[[ 0.          0.        ]\n",
            " [-0.69993085 -0.7142106 ]\n",
            " [-0.         -0.        ]\n",
            " [ 0.7647814  -0.6442898 ]\n",
            " [ 0.         -0.        ]\n",
            " [-0.94481426 -0.3276065 ]]\n",
            "\n",
            "Target Variable or Classes Defined are in Y Variable\n",
            "[0 0 1 1 2 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets take big sample\n",
        "# 100 samples of 3 target classes\n",
        "X, Y = spiral_data(samples = 2,classes = 3)\n",
        "\n",
        "np.random.seed(0)\n",
        "class Layer_Dense:\n",
        "  \n",
        "  # Random Initialization of Weight and Bias\n",
        "  def __init__(self, n_inputs, n_neurons):\n",
        "    print(\"Number of Inputs, n_inputs :\",n_inputs)\n",
        "    print(\"Number of Neurons, n_neurons :\",n_neurons) \n",
        "    print(\"In our case n_neurons is number of outputs\")\n",
        "    \n",
        "    self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
        "    print(\"\\nRandom weights Initialized:\")\n",
        "    print(\"2 output means 2 rows and each row contains 3 input weight as we have 3 neurons\")\n",
        "    print(self.weights)\n",
        "    \n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "    print(\"\\nTwo Output means Two Biases but we are adding zero biases :\")\n",
        "    print(self.biases)\n",
        "  \n",
        "  # Calculation of Output = (input * weight) + bias\n",
        "  def forward(self,inputs):\n",
        "    print(\"\\nOriginal Data before Output Calculation\")\n",
        "    print(inputs)\n",
        "    self.output = np.dot(inputs, self.weights)\n",
        "\n",
        "class Activation_ReLU:\n",
        "  # Forward Pass\n",
        "  def forward(self,inputs):\n",
        "    self.output = np.maximum(0,inputs)\n",
        "\n",
        "# Create a Dense Layer with 2 input features (X and Y) and 3 Output Class\n",
        "dense1 = Layer_Dense(2,3)\n",
        "\n",
        "# Perform a forward pass of out training data through this layer\n",
        "# As we discussed eariler in notebook, Data is in X Variable\n",
        "dense1.forward(X)\n",
        "\n",
        "print(\"\\nOutput after Weight and Bias Calculation\")\n",
        "print(dense1.output)\n",
        "\n",
        "activation_1 = Activation_ReLU()\n",
        "activation_1.forward(dense1.output)\n",
        "\n",
        "print(\"\\nOutput after ReLU Activation Function\")\n",
        "print(activation_1.output)\n",
        "\n",
        "# Observation\n",
        "# As you can see, negative values have been ​clipped​ (modified to be zero). \n",
        "# That’s all there is to the rectified linear activation function used in the hidden layer."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjWVUT_fUaMm",
        "outputId": "c57ec5d8-7ca7-415e-c4c8-3498f1060002"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Inputs, n_inputs : 2\n",
            "Number of Neurons, n_neurons : 3\n",
            "In our case n_neurons is number of outputs\n",
            "\n",
            "Random weights Initialized:\n",
            "2 output means 2 rows and each row contains 3 input weight as we have 3 neurons\n",
            "[[ 0.17640524  0.04001572  0.0978738 ]\n",
            " [ 0.22408931  0.1867558  -0.09772779]]\n",
            "\n",
            "Two Output means Two Biases but we are adding zero biases :\n",
            "[[0. 0. 0.]]\n",
            "\n",
            "Original Data before Output Calculation\n",
            "[[ 0.          0.        ]\n",
            " [-0.47902483 -0.87780136]\n",
            " [-0.         -0.        ]\n",
            " [ 0.97696507  0.2133992 ]\n",
            " [ 0.          0.        ]\n",
            " [-0.63560337  0.7720158 ]]\n",
            "\n",
            "Output after Weight and Bias Calculation\n",
            "[[ 0.          0.          0.        ]\n",
            " [-0.2812084  -0.18310302  0.03890161]\n",
            " [ 0.          0.          0.        ]\n",
            " [ 0.22016223  0.07894751  0.07476425]\n",
            " [ 0.          0.          0.        ]\n",
            " [ 0.06087673  0.11874431 -0.13765632]]\n",
            "\n",
            "Output after ReLU Activation Function\n",
            "[[0.         0.         0.        ]\n",
            " [0.         0.         0.03890161]\n",
            " [0.         0.         0.        ]\n",
            " [0.22016223 0.07894751 0.07476425]\n",
            " [0.         0.         0.        ]\n",
            " [0.06087673 0.11874431 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation Function\n",
        "# y = e^z / sum(e^z)\n",
        "# The first step for us is to “exponentiate” the outputs. \n",
        "# We do this with Euler’s number, ​e, ​which is roughly ​2.71828182846​ and referred to as the “exponential growth” number. \n",
        "\n",
        "# Both the numerator and the denominator of the Softmax function contain ​e​ raised to the power of z​, where ​z​, given means a singular output value\n",
        "# The numerator exponentiates the current output value and the denominator takes a sum of all of the exponentiated outputs for a given sample. \n",
        "\n",
        "layers_output = [1,2,3,4]\n",
        "E = 2.71828182846\n",
        "\n",
        "# For each value in a vector, calculate exponential value\n",
        "exp_value = []\n",
        "for output in layers_output:\n",
        "  exp_value.append(E ** output) # ** signifies power\n",
        "  print('\\nInput :', output)\n",
        "  print('E ** Input :',E ** output)\n",
        "  print('Exponential List :',exp_value)\n",
        "\n",
        "print(\"\\nFinal Exponential List :\",exp_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHAL6qU4Va6K",
        "outputId": "341c64c3-baa9-4f61-ef70-976b10bf1ddf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input : 1\n",
            "E ** Input : 2.71828182846\n",
            "Exponential List : [2.71828182846]\n",
            "\n",
            "Input : 2\n",
            "E ** Input : 7.38905609893584\n",
            "Exponential List : [2.71828182846, 7.38905609893584]\n",
            "\n",
            "Input : 3\n",
            "E ** Input : 20.085536923208828\n",
            "Exponential List : [2.71828182846, 7.38905609893584, 20.085536923208828]\n",
            "\n",
            "Input : 4\n",
            "E ** Input : 54.59815003322094\n",
            "Exponential List : [2.71828182846, 7.38905609893584, 20.085536923208828, 54.59815003322094]\n",
            "\n",
            "Final Exponential List : [2.71828182846, 7.38905609893584, 20.085536923208828, 54.59815003322094]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exponentiation serves multiple purposes. \n",
        "- To calculate the probabilities, we need non-negative values. \n",
        "- Imagine the output as ​[​1, ​2​, -​2​]​ \n",
        "  - Even after normalization, the last value will still be negative since we’ll just divide all of them by their sum. \n",
        "  - A negative probability (or confidence) does not make much sense. \n",
        "  - An exponential value of any number is always non-negative. It returns \n",
        "    - 0 for negative infinity\n",
        "    - 1 for the input of 0\n",
        "    - And increases for positive values\n"
      ],
      "metadata": {
        "id": "A3A4qCZLnqw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layers_output = [1,2,-2,69]\n",
        "E = 2.71828182846\n",
        "\n",
        "# For each value in a vector, calculate exponential value\n",
        "exp_value = []\n",
        "for output in layers_output:\n",
        "  exp_value.append(E ** output) # ** signifies power\n",
        "  print('\\nInput :', output)\n",
        "  print('E ** Input :',E ** output)\n",
        "  print('Exponential List :',exp_value)\n",
        "\n",
        "print(\"\\nFinal Exponential List :\",exp_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Vu3QB6WmbQy",
        "outputId": "b3ac23e9-53f5-42f2-8537-c232fde6fdd1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input : 1\n",
            "E ** Input : 2.71828182846\n",
            "Exponential List : [2.71828182846]\n",
            "\n",
            "Input : 2\n",
            "E ** Input : 7.38905609893584\n",
            "Exponential List : [2.71828182846, 7.38905609893584]\n",
            "\n",
            "Input : -2\n",
            "E ** Input : 0.13533528323651764\n",
            "Exponential List : [2.71828182846, 7.38905609893584, 0.13533528323651764]\n",
            "\n",
            "Input : 69\n",
            "E ** Input : 9.25378172581203e+29\n",
            "Exponential List : [2.71828182846, 7.38905609893584, 0.13533528323651764, 9.25378172581203e+29]\n",
            "\n",
            "Final Exponential List : [2.71828182846, 7.38905609893584, 0.13533528323651764, 9.25378172581203e+29]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The exponential function is a monotonic function. \n",
        "- This means that, with higher input values, outputs are also higher so we won’t change the predicted class after applying it while making sure that we get non-negative values. \n",
        "- It also adds stability to the result as the normalized exponentiation is more about the difference between numbers than their magnitudes.\n",
        "\n",
        "\n",
        "- Once we’ve exponentiated, we want to convert these numbers to a probability distribution (converting the values into the vector of confidences, one for each class, which add up to 1 for everything in the vector). \n",
        "- What that means is that we’re about to **perform a normalization** where we take a given value and divide it by the sum of all of the values. \n",
        "\n",
        "- For our outputs, exponentiated at this stage, that’s what the equation of the **Softmax function** describes next — to take a given exponentiated value and divide it by the sum of all of the exponentiated values. \n",
        "- Since each output value normalizes to a fraction of the sum, all of the values are now in the range of 0 to 1 and add up to 1 — they share the probability of 1 between themselves."
      ],
      "metadata": {
        "id": "DvvMQ1b5nm2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let’s add the sum and normalization to the code:\n",
        "\n",
        "layers_output = [1,2,3,4]\n",
        "E = 2.71828182846\n",
        "\n",
        "# For each value in a vector, calculate exponential value\n",
        "exp_value = []\n",
        "for output in layers_output:\n",
        "  exp_value.append(E ** output)\n",
        "\n",
        "norm_base = sum(exp_value)\n",
        "print(\"Raw Input :\",layers_output)\n",
        "print(\"Exponential Values :\",exp_value)\n",
        "print('Sum of all Exponential Values :',norm_base)\n",
        "\n",
        "# Sum all the values\n",
        "norm_values = []\n",
        "for value in exp_value:\n",
        "  print('\\nInput :',value)\n",
        "  print('Normalized Input (value/sum_values) :',value/norm_base)\n",
        "  norm_values.append(value/norm_base)\n",
        "  print(norm_values)\n",
        "\n",
        "print('\\nFinal Normalized Exponential Values :',norm_values)\n",
        "print('Sum of Final Normalized Values :',sum(norm_values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O6DX7o5nJ2R",
        "outputId": "bc4b84c5-c609-4375-8100-e6c641d240a2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Input : [1, 2, 3, 4]\n",
            "Exponential Values : [2.71828182846, 7.38905609893584, 20.085536923208828, 54.59815003322094]\n",
            "Sum of all Exponential Values : 84.7910248838256\n",
            "\n",
            "Input : 2.71828182846\n",
            "Normalized Input (value/sum_values) : 0.03205860328005693\n",
            "[0.03205860328005693]\n",
            "\n",
            "Input : 7.38905609893584\n",
            "Normalized Input (value/sum_values) : 0.08714431874198689\n",
            "[0.03205860328005693, 0.08714431874198689]\n",
            "\n",
            "Input : 20.085536923208828\n",
            "Normalized Input (value/sum_values) : 0.23688281808986916\n",
            "[0.03205860328005693, 0.08714431874198689, 0.23688281808986916]\n",
            "\n",
            "Input : 54.59815003322094\n",
            "Normalized Input (value/sum_values) : 0.6439142598880871\n",
            "[0.03205860328005693, 0.08714431874198689, 0.23688281808986916, 0.6439142598880871]\n",
            "\n",
            "Final Normalized Exponential Values : [0.03205860328005693, 0.08714431874198689, 0.23688281808986916, 0.6439142598880871]\n",
            "Sum of Final Normalized Values : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can perform the same set of operations with the use of NumPy in the following way:\n",
        "\n",
        "layers_output = [1,2,3,4]\n",
        "print(\"Raw Input :\",layers_output)\n",
        "\n",
        "# First Scaling by calculating exponential values\n",
        "exp_values = np.exp(layers_output)\n",
        "print(\"\\nExpoential Values :\",exp_values)\n",
        "\n",
        "# Now Normalize the exponential values\n",
        "norm_values = exp_values/ np.sum(exp_values)\n",
        "print('\\nFinal Normalized Exponential Values :',norm_values)\n",
        "print('Sum of Final Normalized Values :',sum(norm_values))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pohINtxhqkjm",
        "outputId": "81af992b-fe1c-47eb-8e6b-6657427c845d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Input : [1, 2, 3, 4]\n",
            "\n",
            "Expoential Values : [ 2.71828183  7.3890561  20.08553692 54.59815003]\n",
            "\n",
            "Final Normalized Exponential Values : [0.0320586  0.08714432 0.23688282 0.64391426]\n",
            "Sum of Final Normalized Values : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = np.array([[1,2],[-1,-2]])\n",
        "print(\"Input :\\n\",inputs)\n",
        "\n",
        "# To run in batches \n",
        "exp_values = np.exp(inputs)\n",
        "print(\"\\nExponential Value of Input :\\n\",exp_values)\n",
        "\n",
        "# Now Normalize the exponential values\n",
        "norm_values = exp_values/ np.sum(exp_values,axis=1,keepdims = True) # Axis = 1 means row wise calculation \n",
        "print(\"\\nNormalized Value of Input :\\n\",norm_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq4ALPRyrDVh",
        "outputId": "0dda0d22-7eb5-44aa-bb8d-ecac9c48053b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input :\n",
            " [[ 1  2]\n",
            " [-1 -2]]\n",
            "\n",
            "Exponential Value of Input :\n",
            " [[2.71828183 7.3890561 ]\n",
            " [0.36787944 0.13533528]]\n",
            "\n",
            "Normalized Value of Input :\n",
            " [[0.26894142 0.73105858]\n",
            " [0.73105858 0.26894142]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Two main pervasive challenges with neural networks: \n",
        "  1. Dead neurons\n",
        "  2. Very large numbers (referred to as “exploding” values). \n",
        "\n",
        "- The exponential function used in softmax activation is one of the sources of exploding values. \n",
        "\n",
        "#### What is Dead Neuron ?\n",
        "- When using ReLU, you are using a stepwise function that evaluates to 0 whenever the input is less than or equal to 0. \n",
        "- Because of this piecewise nature, the gradient is 0 if the input is <= 0, since the slope here is 0. \n",
        "- However, if every training example causes a certain neuron to have a negative value (which then becomes 0 after ReLU is applied), then the neuron will never be adjusted, since no matter which training example is selected (or which batch) the gradient on the neuron will be 0. \n",
        "- Thus, the neuron is completely useless- it outputs 0 regardless of which training example comes in, and no matter how much training, it will always output 0 (since its weights never get changed; the gradient is always 0).\n",
        "- In practice, a network with ReLU activations often has a few dead neurons, but a few dead neurons won’t cause too much of a problem. \n",
        "- Too many dead neurons, however, and the neural network loses a lot of its explanatory power."
      ],
      "metadata": {
        "id": "Uf80DjkjvM_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample of exponential outputs\n",
        "print(\"exponential of 1 :\",np.exp(1))\n",
        "print(\"exponential of 10  :\",np.exp(10))\n",
        "print(\"exponential of 100  :\",np.exp(100))\n",
        "print(\"exponential of 1000 cause error  :\",np.exp(1000))\n",
        "\n",
        "# Inference :\n",
        "# It doesn’t take a very large number, in this case, a mere ​1,000,​ to cause an overflow error. \n",
        "# We know the exponential function tends toward 0 as its input value approaches negative infinity, and the output is 1 when the input is 0 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0B12XtLvMoB",
        "outputId": "b7acfaf0-b934-4d60-aa85-97826f1d20d7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exponential of 1 : 2.718281828459045\n",
            "exponential of 10  : 22026.465794806718\n",
            "exponential of 100  : 2.6881171418161356e+43\n",
            "exponential of 1000 cause error  : inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -inf gives the output 0\n",
        "print(np.exp(-np.inf), np.exp(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYIVmQxGw14k",
        "outputId": "5c15bc4a-5f33-444b-f557-08031dba97ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.0 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can use this property to prevent the exponential function from overflowing. \n",
        "- Suppose we subtract the maximum value from a list of input values. \n",
        "- We would then change the output values to always be in a range from some negative value up to 0\n",
        "  - Largest number subtracted by itself returns 0\n",
        "  - Any smaller number subtracted by it will result in a negative number \n",
        "  - After this, number - max(number) we will exponent it. \n",
        "  - So largest number will have value 0 and exponent(0) is 1\n",
        "  - After finding the exponents of each input we will find probabiltity or normalization\n",
        "- With Softmax, thanks to the normalization, we can subtract any value from all of the inputs, and it will not change the output:"
      ],
      "metadata": {
        "id": "euwUcj0OxG56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation Function\n",
        "\n",
        "class Activation_Softmax:\n",
        "  def forward(self, inputs):\n",
        "    print(\"\\nInput :\",inputs)\n",
        "    print(\"\\nMaximum Input :\",np.max(inputs, axis=1, keepdims=True))\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "    print(\"\\nExponent Values of Each Input,logic is exponent((input - max(input)) :\\n\",exp_values)\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "    print(\"\\nProbabiltiy of Each Input :\\n\",probabilities)\n",
        "    self.output = probabilities\n",
        "\n",
        "softmax = Activation_Softmax()\n",
        "softmax.forward([[1,2,3]])\n",
        "print(\"\\nSoftmax Output (Same as Probabilities) :\\n\",softmax.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOt9_pPPtMvu",
        "outputId": "415790c1-0aae-454f-f306-00417ceb7f60"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input : [[1, 2, 3]]\n",
            "\n",
            "Maximum Input : [[3]]\n",
            "\n",
            "Exponent Values of Each Input,logic is exponent((input - max(input)) :\n",
            " [[0.13533528 0.36787944 1.        ]]\n",
            "\n",
            "Probabiltiy of Each Input :\n",
            " [[0.09003057 0.24472847 0.66524096]]\n",
            "\n",
            "Softmax Output (Same as Probabilities) :\n",
            " [[0.09003057 0.24472847 0.66524096]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Another example\n",
        "softmax = Activation_Softmax()\n",
        "softmax.forward([[-1,-2,-3]])\n",
        "print(\"\\nSoftmax Output (Same as Probabilities) :\\n\",softmax.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS0suy_3t894",
        "outputId": "8074b9a4-898b-4f06-8cd2-a79e1c888e6a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input : [[-1, -2, -3]]\n",
            "\n",
            "Maximum Input : [[-1]]\n",
            "\n",
            "Exponent Values of Each Input,logic is exponent((input - max(input)) :\n",
            " [[1.         0.36787944 0.13533528]]\n",
            "\n",
            "Probabiltiy of Each Input :\n",
            " [[0.66524096 0.24472847 0.09003057]]\n",
            "\n",
            "Softmax Output (Same as Probabilities) :\n",
            " [[0.66524096 0.24472847 0.09003057]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Function Overall\n",
        "\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "class Activation_Softmax:\n",
        "    def forward(self, inputs):\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "# Random Samples 3 classes with 2 samples each\n",
        "X, y = spiral_data(samples=2, classes=3)\n",
        "\n",
        "dense = Layer_Dense(2, 3) #  X and Y i.e, 2 Inputs and 3 classes or neurons will initilize random 2 weight and 3 biases (zeroes biases)\n",
        "dense.forward(X) # Returns Weight and Bias Calculation\n",
        "\n",
        "# Initializing Softmax Axtivation Function\n",
        "activation = Activation_Softmax()\n",
        "activation.forward(dense.output)\n",
        "\n",
        "print(activation.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WBIxXuk0GzI",
        "outputId": "c350caa7-4012-4d09-f290-a95dd3006db1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33380836 0.3108572  0.3553344 ]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.34230095 0.32963628 0.32806274]\n",
            " [0.33333334 0.33333334 0.33333334]\n",
            " [0.3207316  0.3653398  0.31392863]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merging both Activation Function - RelU and Softmax\n",
        "\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "    def forward(self, inputs):\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "dense1 = Layer_Dense(2,3)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "print(activation2.output[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXPf1MEjzklp",
        "outputId": "85825d4a-18e4-4beb-8cfb-8c29e1535bc7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.33333334 0.33333334 0.33333334]\n",
            " [0.33334747 0.3333181  0.33333445]\n",
            " [0.3333639  0.3332918  0.33334434]\n",
            " [0.33335617 0.33330873 0.33333513]\n",
            " [0.33335337 0.33331177 0.33333492]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As you can see, the distribution of predictions is almost equal, as each of the samples has ~33% (0.33) predictions for each class. \n",
        "- This results from the random initialization of weights (a draw from the normal distribution, as not every random initialization will result in this) and zeroed biases. \n",
        "- These outputs are also our “confidence scores.” \n",
        "- To determine which classification the model has chosen to be the prediction, we perform an ​argmax​ on these outputs, which checks which of the classes in the output distribution has the highest confidence and returns its index - the predicted class index. \n",
        "- That said, the confidence score can be as important as the class prediction itself. \n",
        "- For example, the argmax of ​[​0.22​, ​0.6​, ​0.18​]​ i​s the same as the argmax for [​0.32​, ​0.36​, ​0.32​]​. In both of these, the argmax function would return an index value of 1 (the 2nd element in Python’s zero-indexed paradigm), but obviously, a 60% confidence is much better than a 36% confidence."
      ],
      "metadata": {
        "id": "zsG530go1rzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Few Observations\n",
        "\n",
        "- We’ve completed what we need for forward-passing data through our model. We used the Rectified Linear (ReLU​) activation function on the hidden layer, which works on a per-neuron basis. \n",
        "- We additionally used the ​Softmax​ activation function for the output layer since it accepts non-normalized values as input and outputs a probability distribution, which we’re using as confidence scores for each class. \n",
        "- Recall that, although neurons are interconnected, they each have their respective weights and biases and are not “normalized” with each other.\n",
        "- As you can see, our example model is currently random. To remedy this, we need a way to calculate how wrong the neural network is at current predictions and begin adjusting weights and biases to decrease error over time. \n",
        "\n",
        "**Thus, our next step is to quantify how wrong the model is through what’s defined as a ​loss function​.**"
      ],
      "metadata": {
        "id": "XcRcrb7OdnmE"
      }
    }
  ]
}