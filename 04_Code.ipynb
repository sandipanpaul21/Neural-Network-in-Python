{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_Code.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNzRyDISjCpsskIBiYFMvDM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanpaul21/Neural-Network-in-Python/blob/main/04_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Activation Function\n",
        "\n",
        "- The activation function is applied to the output of a neuron (or layer of neurons), which modifies outputs. \n",
        "- We use activation functions because if the activation function itself is nonlinear, it allows for neural networks with usually two or more hidden layers to map nonlinear functions. \n",
        "- In general, your neural network will have two types of activation functions. \n",
        "  1. The first will be the activation function used in hidden layers\n",
        "  2. The second will be used in the output layer. \n",
        "  \n",
        "  Usually, the activation function used for hidden neurons will be the same for all of them, but it doesn’t have to.\n",
        "\n",
        "#### Types of Activation Function\n",
        "1. Step Activation Function\n",
        "2. Linear Activation Function\n",
        "3. Sigmoid Activation Function\n",
        "3. Rectifier Linear Activation Function\n",
        "\n",
        "#### 1. The Step Activation Function\n",
        "- Recall the purpose this activation function serves is to mimic a neuron “firing” or “not firing” based on input information. \n",
        "- The simplest version of this is a step function. \n",
        "- Formula wise, \n",
        "        y = 1 if output > 0 else 0 where output = sum(weights * inputs) + bias\n",
        "- In a single neuron, \n",
        "  - Output​ > 0 then Output of Neuron is 1\n",
        "  - otherwise, it will output 0.\n",
        "\n",
        "#### 2. The Sigmoid Activation Function\n",
        "- The problem with the step function is it’s not very informative. \n",
        "- The problem with a step function is \n",
        "  - It’s give less clear to the optimizer what these impacts are because there’s very little information gathered from this function. \n",
        "  - It’s either on (1) or off (0). \n",
        "  - It’s hard to tell how “close” this step function was to activating or deactivating. Maybe it was very close, or maybe it was very far. \n",
        "- In terms of the final output value from the network, it doesn’t matter if it was ​close​ to outputing something else. \n",
        "- Thus, when it comes time to optimize weights and biases, it’s easier for the optimizer if we have activation functions that are more granular and informative.\n",
        "- The original, more granular, activation function used for neural networks was the ​Sigmoid activation function, which looks like:\n",
        "        y = 1 / (1 + e^-x) # e raise to -x\n",
        "  - This function \n",
        "    - Range is from 0 to 1\n",
        "    - Returns a value in the range of 0 for negative infinity, \n",
        "    - Through 0.5 for the input of 0, \n",
        "    - To 1 for positive infinity.\n",
        "\n",
        "#### 3. The Linear Activation Function\n",
        "- A linear function is simply the equation of a line. \n",
        "- It will appear as a straight line when graphed, \n",
        "        y=x here, the output value equals the input.\n",
        "\n",
        "#### 4. The Rectified Linear Activation Function (ReLU)\n",
        "- ReLU means Rectified Linear Units​ activation function\n",
        "- The rectified linear activation function is simpler than the sigmoid. It’s quite literally ​y=x,​ clipped at 0 from the negative side. If ​x​ is less than or equal to ​0,​ then ​y​ is ​0​ — otherwise, ​y​ is equal to ​x​.\n",
        "        y = x if x > 0 else 0 if x <= 0\n",
        "  - This function\n",
        "    - Will return x if x > 0\n",
        "    - Else return 0 if x < = 0\n",
        "\n",
        "#### Why Use Activation Functions?\n",
        "- In most cases, for a neural network to fit a nonlinear function, we need it to contain two or more hidden layers, and we need those hidden layers to use a nonlinear activation function.\n",
        "- A nonlinear function cannot be represented well by a straight line, such as a sine function.\n",
        "- Most of the problems in world are non linear. So to fit a model to a non linear data we use Activation Function like ReLU which help the model to fit to an Non Linear Data"
      ],
      "metadata": {
        "id": "Vy7iv2ZcMxHp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PPQxn9GL9Mi"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}