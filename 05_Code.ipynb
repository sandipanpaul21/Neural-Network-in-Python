{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_Code.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOiL5brWiRzq7sA5OByT1bg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandipanpaul21/Neural-Network-in-Python/blob/main/05_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating Network Error with Loss\n",
        "\n",
        "- With a randomly-initialized model, or even a model initialized with more sophisticated approaches, our goal is to train, or teach, a model over time. To train a model, we tweak the weights and biases to improve the model’s accuracy and confidence. \n",
        "- To do this, we calculate how much error the model has. The ​loss function​, also referred to as the ​**cost function​**\n",
        "- It is the algorithm that quantifies how wrong a model is. ​Loss​ is the measure of this metric. Since loss is the model’s error, we ideally want it to be 0."
      ],
      "metadata": {
        "id": "ZnTY0LGxbcaE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorical Cross-Entropy Loss\n",
        "\n",
        "- Categorical cross-entropy​ is explicitly used to compare a “ground-truth” probability (​y o​r​ “​ ​targets”​ ) and some predicted distribution (​y-hat o​r “​predictions​”)\n",
        "- So it makes sense to use cross-entropy here. \n",
        "- It is also one of the most commonly used loss functions with a softmax activation on the output layer.\n",
        "\n",
        "The formula for calculating the categorical cross-entropy of ​y​ (actual/desired distribution) and y-hat​ (predicted distribution) is: \n",
        "\n",
        "     Loss = - Actual * log(Predicted)\n",
        "\n",
        "- Cross-entropy compares two probability distributions. \n",
        "\n",
        "  In our case, we have a softmax output: softmax_output ​= ​[​0.7​, ​0.1​, ​0.2​]\n",
        "\n",
        "**Which probability distribution do we intend to compare this to?**\n",
        " \n",
        "- We have 3 class confidences in the above output, and let’s assume that the desired prediction is the first class (index 0, which is currently 0.7). \n",
        "- If that’s the intended prediction, then the desired probability distribution is ​[​1​, 0​, ​0​]​. \n",
        "- Cross-entropy can also work on probability distributions like ​[​0.2​, ​0.5​, ​0.3​]​; they wouldn’t have to look like the one above. \n",
        "- That said, the desired probabilities will consist of a 1 in the desired class, and a 0 in the remaining undesired classes. \n",
        "\n",
        "- Arrays or vectors like this are called ​**one-hot**​,​ meaning one of the values is “hot” (on), with a value of 1, and the rest are “cold” (off), with values of 0. \n",
        "- When comparing the model’s results to a one-hot vector using cross-entropy, the other parts of the equation zero out, and the target probability’s log loss is multiplied by 1, making the cross-entropy calculation relatively simple. This is also a special case of the cross-entropy calculation, called categorical cross-entropy."
      ],
      "metadata": {
        "id": "qAh4IbuxbpB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Predicted Output\n",
        "softmax_output = [0.7,0.1,0.2]  \n",
        "\n",
        "# Truth \n",
        "target_output = [1,0,0]\n",
        "\n",
        "# Loss = - Actual * Log(Predicted)\n",
        "loss = -(target_output[0] * math.log(softmax_output[0]) +\n",
        "         target_output[1] * math.log(softmax_output[1]) +\n",
        "         target_output[2] * math.log(softmax_output[2]))\n",
        "\n",
        "print(\"Model Loss :\",loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7_Oy5olc0kF",
        "outputId": "0a328e1a-dadc-482a-be52-9a4079256723"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Loss : 0.35667494393873245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Predicted\n",
        "softmax_output = np.array([[0.7, 0.1,0.2],\n",
        "                           [0.1, 0.5, 0.4],\n",
        "                           [0.02,0.9,0.08]])\n",
        "\n",
        "# Truth\n",
        "target_output = [0,1,1]\n",
        "\n",
        "print(softmax_output[[0,1,2],target_output])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB-sDQYMkJG8",
        "outputId": "ccb9dd31-2220-493b-fafc-f761910c7cb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.7 0.5 0.9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- NumPy lets us index an array in multiple ways. \n",
        "- One of them is to use a list filled with indices and that’s convenient for us — we could use the ​class_targets​ for this purpose as it already contains the list of indices that we are interested in. \n",
        "- The problem is that this has to filter data rows in the array — the second dimension. \n",
        "- To perform that, we also need to explicitly filter this array in its first dimension. This dimension contains the predictions and we, of course, want to retain all. \n",
        "- We can achieve that by using a list containing numbers from 0 through all of the indices. We know we’re going to have as many indices as distributions in our entire batch, so we can use a ​range​()​ instead of typing each value ourselves:"
      ],
      "metadata": {
        "id": "XzWnq9jYlPUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Softmax Output :\\n\",softmax_output)\n",
        "print(\"Length of Softmax Output :\",len(softmax_output))\n",
        "print(\"Range of Softmax Output :\",range(len(softmax_output)))\n",
        "print(\"Target Output :\",target_output)\n",
        "print(\"So for range (0 : n), starting from list 0 to n-1 extract Target Output Position to find Highest Probability\")\n",
        "print(\"Highest Probability Row Wise (by adding target column) : \",softmax_output[range(len(softmax_output)),target_output])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC5naGQnlcBb",
        "outputId": "4d1cb9d9-d445-430a-afad-a3387c50aedd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Softmax Output :\n",
            " [[0.7  0.1  0.2 ]\n",
            " [0.1  0.5  0.4 ]\n",
            " [0.02 0.9  0.08]]\n",
            "Length of Softmax Output : 3\n",
            "Range of Softmax Output : range(0, 3)\n",
            "Target Output : [0, 1, 1]\n",
            "So for range (0 : n), starting from list 0 to n-1 extract Target Output Position to find Highest Probability\n",
            "Highest Probability Row Wise (by adding target column) :  [0.7 0.5 0.9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Since we implemented this to work with sparse labels (as in our training data), we have to add a check if they are one-hot encoded and handle it a bit differently in this new case. \n",
        "- For example above have a list which contains true value [0,1,1] contains list only correct answer and it contains only one list\n",
        "- But suppose same can be represented as one hot encoded true values [ [1,0,0], [0,1,0] , [0,1,0] ] here it is list of list\n",
        "- The check can be performed by counting the dimensions — if targets are single-dimensional (like a list)\n",
        "- But if there are 2 dimensions (like a list of lists), then there is a set of one-hot encoded vectors."
      ],
      "metadata": {
        "id": "VoH6NU_doXWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_output = np.array([[0.7, 0.1,0.2],\n",
        "                           [0.1, 0.5, 0.4],\n",
        "                           [0.02,0.9,0.08]])\n",
        "\n",
        "class_target = np.array([[1,0,0],\n",
        "                        [0,1,0],\n",
        "                        [0,1,0]])\n",
        "\n",
        "print(\"Print Length of Class Target (start from 0, if 3 then length(number of rows) is 4) :\",len(class_target.shape))\n",
        "print(\"Print Range of Class Target :\",range(len(class_target.shape)))\n",
        "\n",
        "# Probabilities for target values\n",
        "# only if categorical labels\n",
        "\n",
        "if len(class_target.shape) == 1:\n",
        "  print(\"Class Target is of One List\")\n",
        "  print(\"List contains True Values\")\n",
        "  correct_confidence = softmax_output[range(len(softmax_output)),class_target]\n",
        "\n",
        "elif len(class_target.shape) == 2:\n",
        "  print(\"\\nClass Target has more than One List\")\n",
        "  print(\"One Hot Encoded Way\")\n",
        "  correct_confidence = np.sum(softmax_output * class_target,axis = 1)\n",
        "  print(\"Choosen Probability from every list :\",correct_confidence)\n",
        "\n",
        "# Losses = -log(probabilities)\n",
        "neg_log = -np.log(correct_confidence)\n",
        "print(\"\\nNegative Log(Choosen Probability) :\",neg_log)\n",
        "\n",
        "# Average of Loss\n",
        "average_loss = np.mean(neg_log)\n",
        "print(\"\\nAverage of Negative Log(Choosen Probability) :\",average_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKiQ7r3nefh8",
        "outputId": "37e28819-ebdf-47e7-abd6-91681230c831"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Print Length of Class Target (start from 0, if 3 then length(number of rows) is 4) : 2\n",
            "Print Range of Class Target : range(0, 2)\n",
            "\n",
            "Class Target has more than One List\n",
            "One Hot Encoded Way\n",
            "Choosen Probability from every list : [0.7 0.5 0.9]\n",
            "\n",
            "Negative Log(Choosen Probability) : [0.35667494 0.69314718 0.10536052]\n",
            "\n",
            "Average of Negative Log(Choosen Probability) : 0.38506088005216804\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The softmax output, which is also an input to this loss function, consists of numbers in the range from 0 to 1 - a list of confidences. \n",
        "- It is possible that the model will have full confidence for one label making all the remaining confidences zero. \n",
        "- Similarly, it is also possible that the model will assign full confidence to a value that wasn’t the target. "
      ],
      "metadata": {
        "id": "tzGOSbgcqJGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If we then try to calculate the loss of this confidence of 0:\n",
        "-np.log(0) # Gives Error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPPvRYoPiUCo",
        "outputId": "5f303615-850a-4fde-a4fa-62fc75a20d8a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "inf"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Before we explain this, we need to talk about ​log(0)​. \n",
        "- From the mathematical point of view, ​log(0) is undefined. We already know the following dependence: if ​y=log(x),​ then ​e​^y​=x​. \n",
        "- The question of what the resulting ​y​ is in ​y=log(0)​ is the same as the question of what’s the ​y​ in ​e​^y​=0​. \n",
        "- In simplified terms, the constant ​e​ to any power is always a positive number, and there is no ​y resulting in ​e​^y=​ 0.​ \n",
        "- This means the ​log(0)​ is undefined. We need to be aware of what the ​log(0)​ is, and “undefined” does not mean that we don’t know anything about it. \n",
        "\n",
        "**Since ​log(0)​ is undefined, what’s the result for a value very close to ​0​?**\n",
        "\n",
        "Ans. What this means is that the limit is negative infinity for an infinitely small ​x,​ where ​x​ never reaches ​0.​"
      ],
      "metadata": {
        "id": "5Jc1Hhxiqhon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We could add a very small value to the confidence to prevent it from being a zero, for example, 1e-7​: 0.000,000,1\n",
        "- np.log(1e-7) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyG4NFi7qf5I",
        "outputId": "232fe5c1-ef0f-452c-eb59-dd2ab82c99ed"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16.11809565095832"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a very small value, one-tenth of a million, to the confidence at its far edge will insignificantly impact the result, \n",
        "# but this method yields an additional 2 issues. \n",
        "\n",
        "# First, in the case where the confidence value is ​1​, adding even smallest value will give probability more than 1 which is not possible:\n",
        "- np.log(1+1e-7) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImynV1Qtrmqq",
        "outputId": "05333bd0-3e5e-4b8c-f66c-9868ec76279a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-9.999999505838704e-08"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- When the model is fully correct in a prediction and puts all the confidence in the correct label, loss becomes a negative value instead of being 0. \n",
        "- The other problem here is shifting confidence towards ​1​, even if by a very small value. \n",
        "- To prevent both issues, it’s better to clip values from both sides by the same number, ​1e-7​ in our case.\n",
        "- That means that the lowest possible value will become ​1e-7​ (like in the demonstration we just performed) but the highest possible value, instead of being ​1+1e-7​, will become ​1-1e-7​ (so slightly less than ​1​):"
      ],
      "metadata": {
        "id": "FGtlHytrdbiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "- np.log(1-1e-7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX167l5ZsAon",
        "outputId": "47c81e00-274f-42a4-fedc-2f02c67d40b1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0000000494736474e-07"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Clipping Example\n",
        "\n",
        "Clip (limit) the values in an array.\n",
        "\n",
        "- Given an interval, values outside the interval are clipped to the interval edges. \n",
        "- For example, if an interval of [0, 1] is specified, values smaller than 0 become 0, and values larger than 1 become 1.\n",
        "\n",
        "      a = np.arange(10)\n",
        "      a # array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
        "      \n",
        "      np.clip(a, 1, 8) # array([1, 1, 2, 3, 4, 5, 6, 7, 8, 8])\n",
        "      np.clip(a, 8, 1) # array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ],
      "metadata": {
        "id": "PAntza_ItpIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This will prevent loss from being exactly ​0​, making it a very small value instead, but won’t make it a negative value and won’t bias overall loss towards ​1.​ \n",
        "\n",
        "      y_pred_clipped ​= ​np.clip(y_pred, ​1e-7​, ​1 ​- ​1e-7​)  # (value,smallest range,highest range)\n",
        "\n",
        "- This method can perform clipping on an array of values, so we can apply it to the predictions directly and save this as a separate array,"
      ],
      "metadata": {
        "id": "S-8CtPJetBPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for Cross Entropy Loss\n",
        "\n",
        "# Common loss class\n",
        "class Loss:\n",
        "  # Calculates the data and regularization losses \n",
        "  # Given model output and ground truth values\n",
        "  def calculate(self,output,y):\n",
        "    # Calculate Sample Loss\n",
        "    sample_losses = self.foward(output,y)\n",
        "    # Average of Loss\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    # Returb Loss\n",
        "    return data_loss\n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss) :\n",
        "  # Forward Pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "    \n",
        "    # Number of samplesin Batch\n",
        "    samples = (y_pred)\n",
        "\n",
        "    # Clip Data to prevent division by 0\n",
        "    # Clip both sides to not drag mean towards any value ​y_pred_clipped ​= ​np.clip(y_pred, ​1e-7​, ​1 ​- ​1e-7​)\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "    \n",
        "    # Probabilities for target values \n",
        "    # only if categorical labels \n",
        "    if len(y_true.shape) == 1:\n",
        "      print(\"Actual Target is of One List\")\n",
        "      print(\"List contains True Values\")\n",
        "      correct_confidence = y_pred_clipped[range(len(samples)),y_true]\n",
        "\n",
        "    elif len(y_true.shape) == 2:\n",
        "      print(\"\\nActual Target has more than One List\")\n",
        "      print(\"One Hot Encoded Way\")\n",
        "      correct_confidence = np.sum(y_pred_clipped * y_true,axis = 1)\n",
        "      print(\"Choosen Probability from every list :\",correct_confidence)\n",
        "\n",
        "    # Losses = -log(probabilities)\n",
        "    negative_log_likelihood = -np.log(correct_confidence)\n",
        "    return negative_log_likelihood"
      ],
      "metadata": {
        "id": "C9fTBHzKwKku"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nnfs   \n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()   # Will set random seed = 0, to make it repeatable\n",
        "\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "    def forward(self, inputs):\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "class Loss:\n",
        "  def calculate(self, output,y):\n",
        "    sample_losses = self.forward(output,y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss\n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss) :\n",
        "  def forward(self, y_pred, y_true):\n",
        "    samples = (y_pred)\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidence = y_pred_clipped[range(len(samples)),y_true]\n",
        "\n",
        "    elif len(y_true.shape) == 2:\n",
        "      correct_confidence = np.sum(y_pred_clipped * y_true,axis = 1)\n",
        "    negative_log_likelihood = -np.log(correct_confidence)\n",
        "    return negative_log_likelihood\n",
        "\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "dense1 = Layer_Dense(2,3)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "print(\"Activation Function Output :\\n\",activation2.output[:5])\n",
        "\n",
        "# Create loss function\n",
        "loss_function = Loss_CategoricalCrossEntropy()\n",
        "\n",
        "# Perform a forward pass through loss function\n",
        "# it takes the output of second dense layer here and returns loss\n",
        "loss = loss_function.calculate(activation2.output,y)\n",
        "\n",
        "print(\"Loss :\",loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_54Pjrzjub4A",
        "outputId": "112ef8b5-f907-400f-f3d5-6d2a4da84521"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation Function Output :\n",
            " [[0.33333334 0.33333334 0.33333334]\n",
            " [0.33331734 0.3333183  0.33336434]\n",
            " [0.3332888  0.33329153 0.33341965]\n",
            " [0.33325943 0.33326396 0.33347666]\n",
            " [0.33323312 0.33323926 0.33352762]]\n",
            "Loss : 1.098445\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we get ​~0.33​ values since the model is random, and its average loss is also not great for these data, as we’ve not yet trained our model on how to correct its errors."
      ],
      "metadata": {
        "id": "v8n1LgC52Icc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Accuracy Calculation\n",
        "- While loss is a useful metric for optimizing a model, the metric commonly used in practice along with loss is the ​accuracy​, which describes how often the largest confidence is the correct class in terms of a fraction. \n",
        "- Conveniently, we can reuse existing variable definitions to calculate the accuracy metric. \n",
        "- We will use the ​argmax ​values from the ​softmax outputs ​and then compare these to the targets. \n"
      ],
      "metadata": {
        "id": "YDDIytfj2bAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Output\n",
        "softmax_output = np.array([[0.7, 0.1,0.2],\n",
        "                           [0.5, 0.1, 0.4],\n",
        "                           [0.02,0.9,0.08]])\n",
        "\n",
        "# Ground Truth\n",
        "class_target = np.array([0,1,1])\n",
        "\n",
        "predictions = np.argmax(softmax_output,axis = 1)\n",
        "print(\"Model Predicted : \",predictions)\n",
        "print(\"Actual Truth : \",class_target)\n",
        "\n",
        "# If targets are one hot encoded - convert them\n",
        "if len(class_target.shape) == 2:\n",
        "  class_target = np.argmax(class_target,axis = 1)\n",
        "\n",
        "# Evaluation Accuracy\n",
        "accuracy = np.mean(predictions == class_target)\n",
        "print(\"Accuracy :\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7JbDLU0zzI4",
        "outputId": "3c8d6791-8fc3-452a-e0ca-e73f80f5cf68"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Predicted :  [0 0 1]\n",
            "Actual Truth :  [0 1 1]\n",
            "Accuracy : 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nnfs   \n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()   # Will set random seed = 0, to make it repeatable\n",
        "\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "class Activation_Softmax:\n",
        "    def forward(self, inputs):\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "class Loss:\n",
        "  def calculate(self, output,y):\n",
        "    sample_losses = self.forward(output,y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    return data_loss\n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss) :\n",
        "  def forward(self, y_pred, y_true):\n",
        "    samples = (y_pred)\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidence = y_pred_clipped[range(len(samples)),y_true]\n",
        "\n",
        "    elif len(y_true.shape) == 2:\n",
        "      correct_confidence = np.sum(y_pred_clipped * y_true,axis = 1)\n",
        "    negative_log_likelihood = -np.log(correct_confidence)\n",
        "    return negative_log_likelihood\n",
        "\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "dense1 = Layer_Dense(2,3)\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "dense2 = Layer_Dense(3, 3)\n",
        "activation2 = Activation_Softmax()\n",
        "\n",
        "dense1.forward(X)\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "print(\"Activation Function Output :\\n\",activation2.output[:5])\n",
        "\n",
        "# Create loss function\n",
        "loss_function = Loss_CategoricalCrossEntropy()\n",
        "\n",
        "# Perform a forward pass through loss function\n",
        "# it takes the output of second dense layer here and returns loss\n",
        "loss = loss_function.calculate(activation2.output,y)\n",
        "\n",
        "print(\"Loss :\",loss)\n",
        "\n",
        "# Accuracy\n",
        "predictions = np.argmax(activation2.output,axis = 1)\n",
        "\n",
        "# If targets are one hot encoded - convert them\n",
        "if len(y.shape) == 2:\n",
        "  y = np.argmax(y,axis = 1)\n",
        "\n",
        "# Evaluation Accuracy\n",
        "accuracy = np.mean(predictions == y)\n",
        "print(\"Accuracy :\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDeq1f4M29av",
        "outputId": "5281e97c-c3e6-4d9f-f6f1-a268310a7d55"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Activation Function Output :\n",
            " [[0.33333334 0.33333334 0.33333334]\n",
            " [0.33331734 0.3333183  0.33336434]\n",
            " [0.3332888  0.33329153 0.33341965]\n",
            " [0.33325943 0.33326396 0.33347666]\n",
            " [0.33323312 0.33323926 0.33352762]]\n",
            "Loss : 1.098445\n",
            "Accuracy : 0.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We perform a forward pass through our network and calculate the metrics to signal if the model is performing poorly, we will embark on optimization in the next chapter!"
      ],
      "metadata": {
        "id": "a8c879cz4iOh"
      }
    }
  ]
}